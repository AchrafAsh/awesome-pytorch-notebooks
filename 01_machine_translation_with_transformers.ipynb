{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_machine_translation_with_transformers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpgCJAqw8jKLYawWAhC1cu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/awesome-pytorch-notebooks/blob/main/01_machine_translation_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO924A6qHPU2"
      },
      "source": [
        "# Machine Translation with Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcFwTVArGaA7"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy  # for tokenization\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence  # padding of every batch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DqDY1d93q5M"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbDqbBhGHVfS"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhRwQLxZ11yx"
      },
      "source": [
        "def tokenizer_en(text:str) -> List[str]:\n",
        "    \"\"\"Tokenize a text\n",
        "    \"\"\"\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYCMLrxcHXa_"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, freq_threshold=1):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        \"\"\"Special words:\n",
        "            PAD = Padding (filler word to make a short sentence the same size as the longest sentence in the batch)\n",
        "            SOS = Start of Sentence\n",
        "            EOC = End of Sentence\n",
        "            UNK = Unknown word\n",
        "        \"\"\"\n",
        "        self.idx_to_str = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.str_to_idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_str)\n",
        "\n",
        "    def build_vocab(self, sentences: List[str]):\n",
        "        frequencies = {}  # store the frequency of each word encountered\n",
        "        idx = 4  # 0, 1, 2 and 3 are already set\n",
        "\n",
        "        for sentence in sentences:\n",
        "            for word in tokenizer_en(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.str_to_idx[word] = idx\n",
        "                    self.idx_to_str[idx] = word\n",
        "                    idx += 1\n",
        "        return\n",
        "\n",
        "    def to_int(self, word:str) -> int:\n",
        "        \"\"\"Transform a word into it's corresponding index.\n",
        "        \"\"\"\n",
        "        if word in self.str_to_idx:\n",
        "            return self.str_to_idx[word]\n",
        "        else:\n",
        "            return self.str_to_idx[\"<UNK>\"]\n",
        "\n",
        "    def __getitem__(self, idx:int) -> str:\n",
        "        if idx in self.idx_to_str:\n",
        "            return self.idx_to_str[idx]\n",
        "        else:\n",
        "            return \"<UNK>\"\n",
        "\n",
        "    def encode(self, sentence:str) -> List[str]:\n",
        "        tokens = tokenizer_en(sentence)\n",
        "        return [\n",
        "            self.str_to_idx[token] if token in self.str_to_idx else self.str_to_idx[\"<UNK>\"]\n",
        "            for token in tokens\n",
        "        ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT59YamRGgn5"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "- Embeddings â†’ encode one-hot-encoded words as continuous vectors to catch semantic (might use pre-trained word2vec for that)\n",
        "- Transformer Block stacked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TL1qyXLG91p"
      },
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trgt_vocab_size, hidden_dim=124, padding_idx=0, word_vectors=None):\n",
        "        if word_vectors not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "            assert hidden_dim == weight.size(0) # TODO: update to make sure the output is the same as the input of the transformer\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim=hidden_dim, padding_idx=padding_idx)\n",
        "        \n",
        "        self.transformer = nn.Transformer(d_model=hidden_dim, nhead=8) # docs: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    \n",
        "    def load_embeddings(self, keyed_vectors):\n",
        "        self.embedd\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src, tgt = self.embedding(src), self.embedding(tgt)\n",
        "        return self.transformer(src, tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDQqPcu6uC3"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jU7k7904V8u"
      },
      "source": [
        "# TODO: download a pre-trained word2vec (a very small one to see if a pre-trained yield better results)\n",
        "!wget "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aTBPcKHU1H"
      },
      "source": [
        "# Load pre-trained word vectors\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\n",
        "weights = torch.FloatTensor(model.vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P6rdFgG4er8"
      },
      "source": [
        "def run(model, dataset, epochs, lr=0.01, weight_decay=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        total_loss = 0\n",
        "        for idx, data in enumerate(dataset):\n",
        "            src, tgt = zip(data)\n",
        "            loss = train(model, src, tgt, optimizer)\n",
        "            total_loss += loss\n",
        "\n",
        "        print(f\"Epoch: [{epoch} / {epochs}] | Loss: {total_loss}\")\n",
        "\n",
        "\n",
        "def train(model, src, tgt, optimizer):\n",
        "    model.train()\n",
        "    output = model(src, tgt)\n",
        "    loss = nn.CrossEntropyLoss()(output, tgt[])\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "    \n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for idx, data in enumerate(dataset):\n",
        "        src, tgt = zip(data)\n",
        "        output = model(src, tgt)\n",
        "        loss = criterion(output, tgt)\n",
        "        total_loss += loss\n",
        "    \n",
        "    print(f\"Total Loss: {total_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY_b6zYC6Rqy"
      },
      "source": [
        "# Building the Transformer from scratch\n",
        "\n",
        "- Attention is all you need: [link to paper]\n",
        "\n",
        "[image of the architecture]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guANqzKl6Ysf"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwapU3Ax6j_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1KokvOF6b17"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljb6h5dJ6kbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhSrN81V6d0I"
      },
      "source": [
        "## Encode / Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9H52uw6lLx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_IpMK7c6ggH"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzK3_5ng6lpZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}