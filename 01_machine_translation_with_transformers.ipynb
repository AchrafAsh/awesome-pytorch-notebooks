{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_machine_translation_with_transformers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyORTXYFO7Ap2PNUivfbMBUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AchrafAsh/awesome-pytorch-notebooks/blob/main/01_machine_translation_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO924A6qHPU2"
      },
      "source": [
        "# Machine Translation with Transformers\n",
        "\n",
        "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"400px\" />\n",
        "\n",
        "Official Pytorch Tutorial: https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcFwTVArGaA7"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence  # padding of every batch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "from typing import List"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NiuHggKCXH0"
      },
      "source": [
        "## The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDiVLmONfp3"
      },
      "source": [
        "### Dataset and Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dpk5gjlCWxB"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpGb9yr4KyRS"
      },
      "source": [
        "fr_tokenizer = get_tokenizer('spacy', language='fr')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjWx3za7Kobo",
        "outputId": "a1a410cd-5be9-451b-b792-198f530b4cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.fr.gz', 'train.en.gz')\n",
        "val_urls = ('val.fr.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.fr.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.fr.gz: 100%|██████████| 604k/604k [00:00<00:00, 17.4MB/s]\n",
            "train.en.gz: 100%|██████████| 569k/569k [00:00<00:00, 8.40MB/s]\n",
            "val.fr.gz: 100%|██████████| 23.0k/23.0k [00:00<00:00, 9.95MB/s]\n",
            "val.en.gz: 100%|██████████| 21.6k/21.6k [00:00<00:00, 3.58MB/s]\n",
            "test_2016_flickr.fr.gz: 100%|██████████| 22.3k/22.3k [00:00<00:00, 8.29MB/s]\n",
            "test_2016_flickr.en.gz: 100%|██████████| 21.1k/21.1k [00:00<00:00, 7.70MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeUsuXLwD_SO"
      },
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "            counter.update(tokenizer(string_))\n",
        "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_enQSO1xKkrc"
      },
      "source": [
        "fr_vocab = build_vocab(train_filepaths[0], fr_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X3Ql86SKlB5"
      },
      "source": [
        "def data_process(filepaths):\n",
        "    raw_fr_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_fr, raw_en) in zip(raw_fr_iter, raw_en_iter):\n",
        "        fr_tensor_ = torch.tensor([fr_vocab[token] for token in fr_tokenizer(raw_fr)],\n",
        "                                dtype=torch.long)\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                                dtype=torch.long)\n",
        "        data.append((fr_tensor_, en_tensor_))\n",
        "    return data"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHYf_Z80GI4Y"
      },
      "source": [
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhAJYMOILKWw"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZg63WQ-LNPC"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UImtRWgLWVG"
      },
      "source": [
        "def generate_batch(data_batch):\n",
        "    fr_batch, en_batch = [], []\n",
        "    for (fr_item, en_item) in data_batch:\n",
        "        fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    return fr_batch, en_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycDFgOCLXjj"
      },
      "source": [
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT59YamRGgn5"
      },
      "source": [
        "## The model\n",
        "\n",
        "- Embeddings → encode one-hot-encoded words as continuous vectors to catch semantic (might use pre-trained word2vec for that)\n",
        "- Transformer Block stacked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TL1qyXLG91p"
      },
      "source": [
        "class Translator(nn.Module):\n",
        "    def __init__(self, \n",
        "                 src_vocab_size: int,\n",
        "                 trgt_vocab_size: int,\n",
        "                 hidden_dim:int=124,\n",
        "                 word_vectors=None):\n",
        "        \n",
        "        super().__init__()\n",
        "        if word_vectors not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "            assert hidden_dim == weight.size(0) # TODO: update to make sure the output is the same as the input of the transformer\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(num_embeddings=src_vocab_size,\n",
        "                                          embedding_dim=hidden_dim)\n",
        "        \n",
        "        self.transformer = nn.Transformer(d_model=hidden_dim, nhead=8) # docs: https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    \n",
        "    def load_embeddings(self, keyed_vectors):\n",
        "        self.embedd\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src, tgt = self.embedding(src), self.embedding(tgt)\n",
        "        return self.transformer(src, tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDQqPcu6uC3"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nys1GAm9Nnij"
      },
      "source": [
        "### Optional: pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jU7k7904V8u"
      },
      "source": [
        "# TODO: download a pre-trained word2vec (a very small one to see if a pre-trained yield better results)\n",
        "!wget "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aTBPcKHU1H"
      },
      "source": [
        "# Load pre-trained word vectors\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')\n",
        "weights = torch.FloatTensor(model.vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-keqRKKNxuy"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyFOur09Ntt0"
      },
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ATTN_DIM = 64\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQvriU1pN9Na"
      },
      "source": [
        "model = Translator()\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"The model has {count_parameters(model):,} parameters 🚀\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P6rdFgG4er8"
      },
      "source": [
        "def run(model:nn.Module,\n",
        "        iterator: DataLoader,\n",
        "        epochs: int,\n",
        "        lr:float=0.01,\n",
        "        weight_decay:float=0.001):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        total_loss = 0\n",
        "        for idx, data in enumerate(dataset):\n",
        "            src, tgt = zip(data)\n",
        "            loss = train(model, src, tgt, optimizer)\n",
        "            total_loss += loss\n",
        "\n",
        "        print(f\"Epoch: [{epoch} / {epochs}] | Loss: {total_loss}\")\n",
        "\n",
        "\n",
        "def train(model, src, tgt, optimizer):\n",
        "    PAD_IDX = en_vocab.stoi['<pad>']\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "    model.train()\n",
        "    output = model(src, tgt)\n",
        "    loss = nn.CrossEntropyLoss()(output, tgt[])\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "    \n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    for idx, data in enumerate(dataset):\n",
        "        src, tgt = zip(data)\n",
        "        output = model(src, tgt)\n",
        "        loss = criterion(output, tgt)\n",
        "        total_loss += loss\n",
        "    \n",
        "    print(f\"Total Loss: {total_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyts29FaORmT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY_b6zYC6Rqy"
      },
      "source": [
        "# Building the Transformer from scratch\n",
        "\n",
        "- Attention is all you need: [link to paper]\n",
        "\n",
        "[image of the architecture]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guANqzKl6Ysf"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwapU3Ax6j_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1KokvOF6b17"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljb6h5dJ6kbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhSrN81V6d0I"
      },
      "source": [
        "## Encode / Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9H52uw6lLx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_IpMK7c6ggH"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzK3_5ng6lpZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}